{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603f74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34314e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers=[16,16]    #[2,3,2] three layers respectively with 2, 3, 2 neurons\n",
    "\n",
    "weights=[]               #this is a list of a few matrices [weight matrices]\n",
    "biases=[]                #this is a list of vectors [biases]\n",
    "\n",
    "activations=[-1]*(len(hidden_layers)+2)    \n",
    "                         #keeps track of activations at \n",
    "                         #each layer (after activated by activation func)\n",
    "num_of_inputs=784                     # def = 784  (MNIST)\n",
    "num_of_outputs=10                      # def = 10  (MNIST)\n",
    "alpha=0.01                                  #learning_rate\n",
    "batch_size=500\n",
    "\n",
    "#generate biases for layer 2,3,....output\n",
    "hidden_layers= hidden_layers + [num_of_outputs]\n",
    "\n",
    "for i in range (len(hidden_layers)):\n",
    "    temp=[]\n",
    "    for k in range (hidden_layers[i]):\n",
    "        temp.append(random.uniform(-0.5,+0.5))\n",
    "        #temp.append(random.uniform(0,1))#/2)\n",
    "    biases.append(np.array(temp))\n",
    "\n",
    "#10 output for MNIST dataset    \n",
    "#generate random weights inside the neural \n",
    "hidden_layers=[num_of_inputs] + hidden_layers        \n",
    "\n",
    "for t in range(len(hidden_layers)-1):\n",
    "    temp=[]\n",
    "    for q in range (hidden_layers[t]*hidden_layers[t+1]):\n",
    "        temp.append(random.uniform(-0.5,+0.5))\n",
    "        #temp.append(random.uniform(0,1))#/2)\n",
    "        \n",
    "    matrix_temp=np.array(temp)\n",
    "    matrix_temp=matrix_temp.reshape((hidden_layers[t],hidden_layers[t+1]))\n",
    "    weights.append(matrix_temp)\n",
    "\n",
    "#print(\"weights:\\n\", weights)\n",
    "#print(\"biases:\\n\", biases)\n",
    "\n",
    "\n",
    "#functions#############################################################################\n",
    "def sigmoid(x):                  # x=numpy so it affects all elements\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "def  feed_forward (input): #input np.array SHAPE=(1,784)\n",
    "    activations[0]=input\n",
    "    for t in range (len(weights)):\n",
    "        next = np.dot(input,weights[t])\n",
    "        next = next + biases[t]\n",
    "        next = sigmoid (next)\n",
    "        activations[t+1]=next   \n",
    "        input=next\n",
    "    return input\n",
    "\n",
    "def feed_backward (t_minus_y,alpha): #t_minus_y is a 1x10 numpy giving errors\n",
    "    \n",
    "    change_in_weights=[] #new list of matrices each element showing the delta_W\n",
    "    change_in_biases=[] #new list of matrices each element showing the delta_B\n",
    "      \n",
    "    \n",
    "    f_of_y_in=activations[-1]\n",
    "    delta_k_all= t_minus_y * f_of_y_in * (1 - f_of_y_in)\n",
    "\n",
    "    change_in_biases.append(alpha * delta_k_all)\n",
    "    \n",
    "    for k in range (2,len(hidden_layers)+1):\n",
    "        z = activations[-k].reshape(1,-1)\n",
    "    \n",
    "    #z = activations[-2].reshape(1,-1)\n",
    "        change_in_weights.append(alpha * np.dot(z.T,delta_k_all.reshape(1,-1)) )    \n",
    "        delta_k_in_all= np.dot(delta_k_all.reshape(1,-1),weights[-k+1].T)\n",
    "        f_of_z_in=activations[-k]\n",
    "   #     f_of_z_in=activations[-2]\n",
    "        delta_k_all= delta_k_in_all * f_of_z_in * (1- f_of_z_in)\n",
    "   #     delta_j_all= delta_j_in_all * f_of_z_in * (1- f_of_z_in)\n",
    "    \n",
    "        if k<len(hidden_layers):\n",
    "            change_in_biases.append(alpha * delta_k_all)\n",
    "   #     z = activations[-3].reshape(1,-1)\n",
    "   #     change_in_weights.append(alpha * np.dot(z.T,delta_j_all.reshape(1,-1)) )\n",
    "        \n",
    "   # print(\"weights change at this step:\", change_in_weights)\n",
    "    \n",
    "    return change_in_weights, change_in_biases\n",
    "\n",
    "\n",
    "def weight_bias_updater (tuple):  #(weights_changes, biases_changes)\n",
    "    w=tuple[0]\n",
    "    w.reverse()\n",
    "    b=tuple[1]\n",
    "    b.reverse()\n",
    "    \n",
    "    for t in range (len(b)):\n",
    "        biases[t] = biases[t]+b[t]\n",
    "        weights[t]= weights[t]+ w[t]      \n",
    "\n",
    "def MSE(target, output):\n",
    "    return np.average((target - output)**2)\n",
    "\n",
    "def One_Hot_Encoding(int):\n",
    "    if int==0:           return np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "    elif int==1:         return np.array([0,1,0,0,0,0,0,0,0,0])\n",
    "    elif int==2:         return np.array([0,0,1,0,0,0,0,0,0,0]) \n",
    "    elif int==3:         return np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "    elif int==4:         return np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "    elif int==5:         return np.array([0,0,0,0,0,1,0,0,0,0])  \n",
    "    elif int==6:         return np.array([0,0,0,0,0,0,1,0,0,0])\n",
    "    elif int==7:         return np.array([0,0,0,0,0,0,0,1,0,0])\n",
    "    elif int==8:         return np.array([0,0,0,0,0,0,0,0,1,0])\n",
    "    elif int==9:         return np.array([0,0,0,0,0,0,0,0,0,1])\n",
    "    \n",
    "def min_max_scale (array):\n",
    "    min=np.min(array)\n",
    "    max=np.max(array)\n",
    "    denom= max - min\n",
    "    if max==0:\n",
    "        return (np.array([0]*len(array)))\n",
    "    else:\n",
    "        return (array - min)/denom\n",
    "  \n",
    "\n",
    "def batch_adder (tup1, tup2):\n",
    "  w1 = tup1[0]\n",
    "  w2 = tup2[0]\n",
    "  b1 = tup1 [1]\n",
    "  b2 = tup2 [1]\n",
    "  w3=[] \n",
    "  b3=[]\n",
    "\n",
    "  for i in range (len(w1)):\n",
    "    w3.append(w1[i]+w2[i])\n",
    "    b3.append(b1[i]+b2[i])\n",
    "\n",
    "  return (w3,b3)\n",
    "\n",
    "def batch_list_adder (list):\n",
    "  my_list=copy.copy(list)\n",
    "  for s in range (len(my_list)-1):\n",
    "       my_list[s+1]=batch_adder(my_list[s],my_list[s+1])\n",
    "\n",
    "  return my_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0688d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('mnist_train.csv') \n",
    "#df_train=pd.read_csv('mnist_train.csv')[0:100]\n",
    "\n",
    "df_train_labels= df_train['label'].values\n",
    "df_train_labels2=np.array([]) #one hot encoded\n",
    "for i in range (len(df_train_labels)):\n",
    "    df_train_labels2 = np.append (df_train_labels2, One_Hot_Encoding(df_train_labels[i]))\n",
    "    \n",
    "#df_train_labels3 = df_train_labels2.reshape(60000,10)   \n",
    "df_train_labels3 = df_train_labels2.reshape(60000,10) \n",
    "\n",
    "df_train_features = df_train.drop(columns=[\"label\"])\n",
    "df_train_features = df_train_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56cde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "df_train_features_scaled=np.array([])\n",
    "for t in range (784):\n",
    "    df_train_features_scaled=np.append(df_train_features_scaled,min_max_scale(df_train_features[:,t]))\n",
    "\n",
    "df_train_features_scaled = df_train_features_scaled.reshape(784,60000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784c438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-06aa0043dc3d>:44: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 completed . . . squared error:  5768.579565772131\n",
      "epoch  1 completed . . . squared error:  5332.489361532528\n"
     ]
    }
   ],
   "source": [
    "alpha=0.001\n",
    "batch_size=500\n",
    "for i in range (2):  #epochs\n",
    "    sum=0\n",
    "    for t in range(0,60000,batch_size):\n",
    "        list_of_change_tuples=[]\n",
    "        #print('new_batch starting ... \\n:')\n",
    "        for b in range(batch_size):\n",
    "          #print(t+b)\n",
    "          input=  df_train_features[t+b]\n",
    "          #print(train_features[t], train_labels[t])\n",
    "          output= feed_forward(input)\n",
    "          error = df_train_labels3[t+b] - output\n",
    "          change_tuple = feed_backward(error,alpha)\n",
    "          list_of_change_tuples.append(change_tuple)\n",
    "          #print(change_tuple)\n",
    "          sum+=MSE(df_train_labels3[t+b],output)\n",
    "            \n",
    "\n",
    "        all_changes= batch_list_adder (list_of_change_tuples)\n",
    "        weight_bias_updater(all_changes)\n",
    "\n",
    "    print('epoch ', i, 'completed . . . squared error: ', sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57628484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-06aa0043dc3d>:44: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.8817666666666667\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for t in range (60000):\n",
    "    if np.argmax(feed_forward(df_train_features[t]))== df_train_labels[t]:\n",
    "        acc+=1\n",
    "print(\"acc = \", acc/60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ae6e973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(feed_forward(df_train_features[50000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8911fa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(feed_forward(df_train_features[30000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d8ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('mnist_test.csv') \n",
    "#df_train=pd.read_csv('mnist_train.csv')[0:100]\n",
    "\n",
    "df_test_labels= df_test['label'].values\n",
    "df_test_features = df_test.drop(columns=[\"label\"])\n",
    "df_test_features = df_test_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9408a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-06aa0043dc3d>:44: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.8809\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for t in range (10000):\n",
    "    if np.argmax( feed_forward(df_test_features[t])      )== df_test_labels[t]:\n",
    "        acc+=1\n",
    "print(\"acc = \", acc/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46dbb4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-401f473addbc>:42: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bef0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r=3719\n",
    "#c=df_test.loc[r].values\n",
    "#rl=c[0]\n",
    "#c=np.delete(c, [0])\n",
    "#p=c.reshape((28,28))\n",
    "#plt.imshow(p, cmap='gray')\n",
    "#plt.show()\n",
    "#print(np.argmax(feed_forward(df_test_features[r])))\n",
    "#print(df_test_labels[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5fa83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc=0\n",
    "#for t in range (10000):\n",
    " #   if np.argmax( feed_forward(df_test_features[t])      )!= df_test_labels[t]:\n",
    "#        rl=df_test_labels[t]\n",
    "#        c=df_test_features[t]\n",
    "#        p=c.reshape((28,28))\n",
    "#        plt.imshow(p, cmap='gray')\n",
    "#        plt.show()\n",
    " #       print(\"NN output: \", np.argmax(feed_forward(df_test_features[t])))\n",
    " #       print(\"Label    : \", df_test_labels[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a9d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
